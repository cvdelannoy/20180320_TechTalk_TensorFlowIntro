{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Animal classification: a feed-forward neural network\n",
    "\n",
    "This is one of them problems that only occurs to the guy in study material. You know, the guy who had a wheel barrow full of melons back in math? He's at it again and now has a zoo of animals for which he'd like too make a classifier. He decides to go for a feed-forward neural network, as he has a lot of labeled examples already. Let's get started!\n",
    "\n",
    "First, import your libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "tf.set_random_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate your zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_animals(nb, hairy, nb_legs, weight_mean, weight_sd, label):\n",
    "    weight_vec = np.random.normal(weight_mean, weight_sd, size=(nb,1))\n",
    "    categorical_properties = np.tile(np.array([label, hairy, nb_legs]), (nb, 1))\n",
    "    features = np.concatenate([categorical_properties, weight_vec], axis=1)\n",
    "    return features\n",
    "\n",
    "\n",
    "ducks = generate_animals(50, 0, 2, 1, 0.43, 1)\n",
    "cats = generate_animals(50, 1, 4, 4, 0.45, 2)\n",
    "dogs = generate_animals(50, 1, 4, 6, 6, 3)\n",
    "bats = generate_animals(50, 1, 2, 1, 0.5, 4)\n",
    "nb_classes = 4\n",
    "\n",
    "animals = np.concatenate([ducks, cats, dogs, bats], axis=0)\n",
    "training, test = train_test_split(animals, test_size=0.20)\n",
    "\n",
    "test_features = test[:, 1:]\n",
    "test_labels_int = test[:, 0].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...500 of each...how does he keep getting in these situations?! Anyway, we have a nice test and training dataset now. Let's see what we can do with this.\n",
    "\n",
    "TensorFlow needs to know what size of input to expect, so define some so called 'placeholders' for that. For convenience we made sure that input is always going to be the same size (1/5th of the full dataset), so that goes into our placeholder too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_examples = int(animals.shape[0] * 0.2)\n",
    "x = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[nb_examples, nb_classes])\n",
    "xl = tf.split(x, nb_examples, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for the model weights! Initialize them at random, biases can be initialized as 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_size=32\n",
    "\n",
    "A1 = tf.Variable(tf.random_normal((3, hidden_layer_size), stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros((1, hidden_layer_size),  dtype=tf.float32))\n",
    "\n",
    "A2 = tf.Variable(tf.random_normal((hidden_layer_size, hidden_layer_size), stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros((1, hidden_layer_size), dtype=tf.float32))\n",
    "\n",
    "A3 = tf.Variable(tf.random_normal((hidden_layer_size, nb_classes), stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros((1, nb_classes), dtype=tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the graph by linking placeholder, weight matrices and weights to eachother. We're going to provde the network with a batch of examples at once (see here why: http://ruder.io/optimizing-gradient-descent/), so the list comprehensions iterate through those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = [tf.nn.sigmoid(tf.add(tf.matmul(xc, A1), b1)) for xc in xl]\n",
    "h2 = [tf.nn.sigmoid(tf.add(tf.matmul(h1c,A2), b2)) for h1c in h1]\n",
    "y_hat = [tf.add(tf.matmul(h2c, A3), b3) for h2c in h2]\n",
    "\n",
    "y_hat = tf.squeeze(tf.stack(y_hat, axis=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost there! Just need to define what you want to optimize (cross entropy is a good choice, see here: http://neuralnetworksanddeeplearning.com/chap3.html) and what algorithm you want to use to optimize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_hat))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We told Tensorflow what to do, now it's going to all of it outside Python (in C), in a \"session\" since it's outside Python it's hard to peek inside while it's running if you haven't configured it to do so. We'll be sure to print some result every now and then to see if we're on the right track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test accuracy: 0.3\n",
      "epoch: 100, test accuracy: 0.425\n",
      "epoch: 200, test accuracy: 0.7\n",
      "epoch: 300, test accuracy: 0.675\n",
      "epoch: 400, test accuracy: 0.675\n",
      "epoch: 500, test accuracy: 0.675\n",
      "epoch: 600, test accuracy: 0.975\n",
      "epoch: 700, test accuracy: 0.675\n",
      "epoch: 800, test accuracy: 0.9\n",
      "epoch: 900, test accuracy: 0.975\n",
      "epoch: 1000, test accuracy: 0.975\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(1001):\n",
    "        for _, idx in KFold(n_splits=4, shuffle=True).split(training):\n",
    "            features = training[idx][:,1:]\n",
    "            labels = training[idx][:,0]\n",
    "            labels = np.eye(nb_classes + 1)[labels.astype(int)].astype(float)[:,1:]\n",
    "            yh, _ = session.run([y_hat, optimizer], feed_dict={\n",
    "                x: features,\n",
    "                y: labels\n",
    "            })\n",
    "        if not epoch % 100:\n",
    "            pred = session.run(y_hat, feed_dict={\n",
    "                x: test_features\n",
    "            })\n",
    "\n",
    "            # print(test_y_hat)\n",
    "            pred_int = np.argmax(pred, axis=1) + 1\n",
    "            test_acc = np.mean(pred_int.astype(int) == test_labels_int)\n",
    "            print('epoch: {epoch}, test accuracy: {test_acc}'.format(test_acc=test_acc, epoch=epoch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good score, weird text book guy triumphs again! Play with your newly constructed network a little; you can implement one of many gradient descent flavors included in TensorFlow to train your network for example, or plot some metrics. Or make up another animal. See you next time text book guy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
